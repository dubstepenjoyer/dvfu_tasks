{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64acc7fb-59de-4ebc-b82f-afb53cc852d2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install fiftyone --user\n",
    "import fiftyone\n",
    "# You don't have to run this cell, if you already have dataset on machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3379e5ef-2708-48c0-a8ed-95eda57a8aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "% pip install tf-models-official\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import tensorflow as tf\n",
    "import tqdm.notebook as tqdm\n",
    "import tensorflow_models as tfm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "299eafa0-89b1-46ba-b659-1bd69880b105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "GPU enable\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        print('GPU enable')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3588f5af-a33c-4638-90d8-5cb912535790",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c468a19b-1395-4afc-b0c5-a799160ea671",
   "metadata": {},
   "source": [
    "Load and prepare your dataset. It should consist of at least 50k images from any openimages split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc874386-9591-4f66-bb9a-116f2ec4ff52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'train' to 'C:\\Users\\dmitry\\fiftyone\\open-images-v7\\train' if necessary\n",
      "Necessary images already downloaded\n",
      "Existing download of split 'train' is sufficient\n",
      "Loading existing dataset 'open-images-v7-train-70000'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
     ]
    }
   ],
   "source": [
    "dataset = fiftyone.zoo.load_zoo_dataset('open-images-v7', split='train', label_types=['classifications'], max_samples=70000)\n",
    "# You don't have to run this cell, if you already have dataset on machine\n",
    "# If you don't have dataset, run this cell and then move directory with images near to this file\n",
    "# On Windows it will be like this \"C:\\Users\\user\\fiftyone\\open-images-v7\\train\\data\"\n",
    "# You have to move the \"data\" directory to this notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = 'data'\n",
    "images = os.listdir(f'{root_path}/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = []\n",
    "\n",
    "for filename in images:\n",
    "    file = f'{root_path}/{filename}'\n",
    "    filepaths.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_data = pd.DataFrame(\n",
    "        filepaths,\n",
    "        columns=['filepaths']\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepaths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/000002b66c9c498e.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/000002b97e5471a0.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/000002c707c9895e.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/0000048549557964.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/000004f4400f6ec5.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>data/0000071d71a0a6f6.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>data/000013ba71c12506.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>data/000018acd19b4ad3.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>data/00001bc2c4027449.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>data/00001bcc92282a38.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   filepaths\n",
       "0  data/000002b66c9c498e.jpg\n",
       "1  data/000002b97e5471a0.jpg\n",
       "2  data/000002c707c9895e.jpg\n",
       "3  data/0000048549557964.jpg\n",
       "4  data/000004f4400f6ec5.jpg\n",
       "5  data/0000071d71a0a6f6.jpg\n",
       "6  data/000013ba71c12506.jpg\n",
       "7  data/000018acd19b4ad3.jpg\n",
       "8  data/00001bc2c4027449.jpg\n",
       "9  data/00001bcc92282a38.jpg"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2689c2fb-ffbe-49b9-b43f-d22ee4612786",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Image augmentation and crops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a9ea72-b879-4db6-ab9f-d1b60d1c8430",
   "metadata": {},
   "source": [
    "Implement image augmentation and crops as described in DINO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e1f110-ee79-4493-8831-a0920e8e55c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_dataset = tf.data.Dataset.from_tensor_slices(pd_data.filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df64afaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geometric_augmentation(image, size, crop_scale):\n",
    "    ratio = np.random.uniform(0.4, 1)\n",
    "    scale = np.random.uniform(crop_scale[0], crop_scale[1])\n",
    "\n",
    "    crop_size = size * scale\n",
    "    crop_size = np.sqrt(crop_size ** 2 * ratio), np.sqrt(crop_size ** 2 / ratio)\n",
    "\n",
    "    image = tf.image.random_crop(image, size=(crop_size, crop_size, 3))\n",
    "    image = tf.image.resize(image, size=(size, size))\n",
    "    image = tf.image.flip_left_right(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d460eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_transform(image):\n",
    "    p1 = np.random.random()\n",
    "    p2 = np.random.random()\n",
    "    if p1 >= 0.2:\n",
    "        image = tf.image.adjust_brightness(image, 0.4)\n",
    "        image = tf.image.adjust_contrast(image, 0.4)\n",
    "        image = tf.image.adjust_saturation(image, 0.2)\n",
    "        image = tf.image.adjust_hue(image, 0.1)\n",
    "\n",
    "    if p2 >= 0.8:\n",
    "        image = tf.image.rgb_to_grayscale(image)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c149d5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_filter(image, probability): # global_transfo1_extra\n",
    "    p = np.random.random()\n",
    "\n",
    "    if p >= probability:\n",
    "        image = tfm.vision.augment.gaussian_filter2d(image, filter_shape=9, sigma=(0.1, 2.0))\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217e7de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_solarize(image, threshold=128, p=0.2):\n",
    "    if np.random.random() > p:\n",
    "        return image\n",
    "    \n",
    "    mask = image > threshold\n",
    "    masked_image = tf.where(mask, 255 - image, image)\n",
    "\n",
    "    return masked_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c730c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(image):\n",
    "    return tf.image.per_image_standardization(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354e03d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dino_augmentation(image, global_crops_scale=(0.14, 1), local_crops_scale=(0.05, 0.4), local_crops_number=3, global_crops_size=224, local_crops_size=96):\n",
    "    output = {}\n",
    "\n",
    "    # global crops\n",
    "    im1_base = geometric_augmentation(image, global_crops_size, global_crops_scale)\n",
    "    global_crop_1 = normalize(gaussian_filter(color_transform(im1_base), 0))\n",
    "\n",
    "    im2_base = geometric_augmentation(image, global_crops_size, global_crops_scale)\n",
    "    global_crop_2 = normalize(random_solarize(gaussian_filter(color_transform(im2_base), 0.9)))\n",
    "\n",
    "    output['global_crops'] = [global_crop_1, global_crop_2]\n",
    "\n",
    "    output['global_crops_teacher'] = [global_crop_1, global_crop_2]\n",
    "\n",
    "    local_crops = [\n",
    "            gaussian_filter(geometric_augmentation(image, local_crops_size, local_crops_scale), 0.5) for _ in range(local_crops_number)\n",
    "        ]\n",
    "    \n",
    "    output[\"local_crops\"] = local_crops\n",
    "    output[\"offsets\"] = ()\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8187b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataset = paths_dataset.map(lambda x: dino_augmentation(tf.io.decode_image(tf.io.read_file(x)), expand_animations=False, channels=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d5bd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_image_dataset = image_dataset.batch(64) # мб не 64..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3209457-198b-4ff5-87d4-95c03a500317",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fab9bb-2588-44dc-9516-5c945ffc370d",
   "metadata": {},
   "source": [
    "Implement the losses and loss additions. DINO, koleo, sinkhorn_knopp, softmax_centering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d510cb4-b184-424f-8cac-a52dba0bbe20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "714bddcc-c56d-4e61-9147-054f5d7f8e7e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21a2f49-08a6-479a-8db7-a52d4db40d56",
   "metadata": {},
   "source": [
    "Implement transofmer model as described in DINO, including the DINO head. Don't forget the teacher weights update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb26115f-5c0e-47a4-b72d-05aa9f940ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1025d0d6-3b43-440a-8791-27b3e208e415",
   "metadata": {},
   "source": [
    "Since we will not be training the backbone, extract features from your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2b4b64-ee2e-4a2a-8d81-161f8bf1aae4",
   "metadata": {},
   "source": [
    "Run the backbone on the images and save the extracted features. Don't forget to process the images. Images don't have to be of the same size, though it would be faster if they were. If the images don't fit in memory, lazily load them from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3449274c-3df3-48b6-afba-e2ad885b4085",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c985c7d-9123-477e-9988-15ff5a7b26eb",
   "metadata": {},
   "source": [
    "Split your data (extracted features and labels) into train and test subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289a1234-6c10-4fd4-ae37-945ec75d6f19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1a65639-50d0-4f89-8bfe-33b7a483e406",
   "metadata": {},
   "source": [
    "Prepare `tf.data.Dataset` or some other way for the data to be used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07849c2e-c7d1-4620-9590-962fa554ed8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = tf.data.Dataset.from_tensor_slices(X_train)\n",
    "train_y = tf.data.Dataset.from_tensor_slices(y_train)\n",
    "train_dataset = tf.data.Dataset.zip((train_X, train_y)).batch(128)\n",
    "\n",
    "test_X = tf.data.Dataset.from_tensor_slices(X_test)\n",
    "test_y = tf.data.Dataset.from_tensor_slices(y_test)\n",
    "test_dataset = tf.data.Dataset.zip((test_X, test_y)).batch(128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f74abd-ce1a-4a68-be6c-89b1abd0caa5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78885e31-c91e-4ee5-acbf-00a107069da9",
   "metadata": {},
   "source": [
    "Train the model as described in DINO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc94f7a-d2b0-48bf-9f0b-df5d17e01e78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a30787b-7ac4-4dd3-9ea5-62c94e9aa628",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3980f062-220a-4a11-b9f5-3b0e41804533",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6008a2-9882-44f2-946c-23827aa5fa49",
   "metadata": {},
   "source": [
    "Show that features extracted by your model are similar for similar images and different for others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4ddf9e-5710-4c7f-ac58-4d94742c0d14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd49c775-eded-4a49-b7f4-9903185a7b9d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726c617b-7110-4761-9806-975d5f84bf41",
   "metadata": {},
   "source": [
    "Train a simple classification model on top of your extracted features on some simple dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bbbaf8-68f6-4aa1-af37-f29b1edae464",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
