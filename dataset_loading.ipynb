{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "GPU enable\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        print('GPU enable')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analog ```tf.keras.utils.image_dataset_from_directory```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './archive/'\n",
    "train_path = f'{path}train/'\n",
    "test_path = f'{path}test/'\n",
    "valid_path = f'{path}valid/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_id = {}\n",
    "\n",
    "def pd_from_path(directory: str) -> pd.DataFrame:\n",
    "    global class_to_id\n",
    "    data = []\n",
    "    for path, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(('.jpg', '.png', '.jpeg', '.bmp', '.gif')):\n",
    "                class_name = os.path.basename(path)\n",
    "                if class_name not in class_to_id:\n",
    "                    class_to_id[class_name] = len(class_to_id)\n",
    "                data.append((f'{path}/{file}', class_to_id[class_name]))\n",
    "\n",
    "    return pd.DataFrame(data, columns=['imagepath', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deleted = []\n",
    "for file in pd_from_path(path).imagepath:\n",
    "    try:\n",
    "      img_bytes = tf.io.read_file(file)\n",
    "      decoded_img = tf.io.decode_image(img_bytes)\n",
    "    except Exception:\n",
    "      deleted.append(file)\n",
    "      os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_load_image(image_path: str, crop_size: tuple[int, int] = (256, 256)):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.io.decode_image(image, expand_animations=False, channels=3)\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "    image = tf.image.resize(image, size=[*crop_size])\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_zip(df: pd.DataFrame, crop_size: tuple[int, int] = (256, 256), batch_size: int = 32):\n",
    "    image_dataset = tf.data.Dataset.from_tensor_slices(df.imagepath)\n",
    "    label_dataset = tf.data.Dataset.from_tensor_slices(df.label)\n",
    "    dataset = tf.data.Dataset.zip((image_dataset.map(lambda x: tf_load_image(x, crop_size)), label_dataset))\n",
    "    return dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_from_directory(\n",
    "        directory: str = './',\n",
    "        batch_size: int = 32,\n",
    "        image_size: tuple[int, int] = (256, 256),\n",
    "        shuffle: bool = True,\n",
    "        seed: int = None,\n",
    "        validation_split: float = None,\n",
    "        subset: str = None\n",
    ") -> tf.data.Dataset:\n",
    "    \n",
    "    df = pd_from_path(directory)\n",
    "    \n",
    "    if shuffle:\n",
    "        df = df.sample(frac=1, random_state=seed)\n",
    "    \n",
    "    if validation_split:\n",
    "        index = round(len(df) * validation_split)\n",
    "        train_dataset = df[index:]\n",
    "        valid_dataset = df[:index]\n",
    "\n",
    "        if subset == 'training':\n",
    "            df = train_dataset\n",
    "        elif subset == 'validation':\n",
    "            df = valid_dataset\n",
    "        elif subset == 'both':\n",
    "            return extract_and_zip(train_dataset), extract_and_zip(valid_dataset)\n",
    "    \n",
    "    \n",
    "    return extract_and_zip(df, image_size, batch_size)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset_from_directory(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input((256, 256, 3), name='input')\n",
    "x = tf.keras.layers.Conv2D(32, 3, padding='same')(inputs)\n",
    "x = tf.keras.layers.PReLU()(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(64)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=inputs, outputs=x, name='ciaonima-1')\n",
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics='accuracy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2809/2809 [==============================] - 231s 81ms/step - loss: nan - accuracy: 0.0019\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x176e5186560>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path: str, batch_size: int, image_size: tuple[int, int], shuffle: bool, split: str) -> tuple[tf.data.Dataset, dict[int, str]]:\n",
    "    '''Given a `path` to a csv index file loads one of the dataset splits. Paths in the index are assumed to be relative to the csv file. The file contains three columns: \"filepaths\", \"labels\" and \"data set\", path to the image, image label and dataset split respectively.\n",
    "\n",
    "    Arguments:\n",
    "        path: path to the csv index file\n",
    "        batch_size: size of batches in the dataset\n",
    "        image_size: size to resize the images to\n",
    "        shuffle: whether to shuffle the index. If False original index order is preserved\n",
    "        split: split to use. One of \"train\", \"valid\" or \"test\"\n",
    "\n",
    "    Returns:\n",
    "        The loaded dataset\n",
    "        A dictionary mapping class indices to class names'''\n",
    "    data = pd.read_csv(path)\n",
    "    data.filepaths = [os.path.join(os.path.dirname(path), file).replace('\\\\', '/') for file in data.filepaths.tolist()]\n",
    "    classes = pd.unique(data.labels)\n",
    "    class_to_id = {name: i for i, name in enumerate(classes)}\n",
    "    data.insert(3, 'labels_id', [class_to_id[name] for name in data.labels], True)\n",
    "\n",
    "    if shuffle:\n",
    "        data = data.sample(frac=1)\n",
    "    \n",
    "    if split:\n",
    "        data = data[data['data set'] == split]\n",
    "\n",
    "    image_dataset = tf.data.Dataset.from_tensor_slices(data.filepaths)\n",
    "    label_dataset = tf.data.Dataset.from_tensor_slices(data.labels_id)\n",
    "    \n",
    "    dataset = tf.data.Dataset.zip((image_dataset.map(lambda x: tf_load_image(x, image_size)), label_dataset))\n",
    "\n",
    "    return dataset.batch(batch_size), {i: name for i, name in enumerate(classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2, cindex = load_dataset('./archive/new_birds.csv', 32, (256, 256), False, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs2 = tf.keras.layers.Input((256, 256, 3), name='input')\n",
    "x2 = tf.keras.layers.Conv2D(32, 3, padding='same')(inputs2)\n",
    "x2 = tf.keras.layers.PReLU()(x2)\n",
    "x2 = tf.keras.layers.Flatten()(x2)\n",
    "x2 = tf.keras.layers.Dense(64)(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = tf.keras.Model(inputs=inputs2, outputs=x2, name='ciaonima-10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics='accuracy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2809/2809 [==============================] - 232s 82ms/step - loss: nan - accuracy: 0.0135\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a4ee919990>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(dataset2, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interface ```tf.keras.utils.Sequence```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetFromDirectory(tf.keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self,\n",
    "                 directory: str = './',\n",
    "                 batch_size: int = 32,\n",
    "                 image_size: tuple[int, int] = (256, 256),\n",
    "                 shuffle: bool = True,\n",
    "                 seed: int = None,\n",
    "                 validation_split: float = None,\n",
    "                 subset: str = None,\n",
    "                 augmentation: bool = True\n",
    "                ):\n",
    "        self.directory = directory\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.shuffle = shuffle\n",
    "        self.seed = seed\n",
    "        self.validation_split = validation_split\n",
    "        self.subset = subset\n",
    "        self.augmentation = augmentation\n",
    "\n",
    "        self.dataframe = self.pd_from_path(self.directory)\n",
    "        \n",
    "        if self.shuffle:\n",
    "            self.dataframe = self.dataframe.sample(frac=1, random_state=self.seed)\n",
    "\n",
    "        if self.validation_split:\n",
    "            index = round(self.n * self.validation_split)\n",
    "            train_dataset = self.dataframe[index:]\n",
    "            valid_dataset = self.dataframe[:index]\n",
    "\n",
    "            if self.subset == 'training':\n",
    "                self.dataframe = train_dataset\n",
    "            elif self.subset == 'validation':\n",
    "                self.dataframe = valid_dataset\n",
    "        \n",
    "        self.n = len(self.dataframe)\n",
    "\n",
    "    def pd_from_path(self, directory: str = './') -> pd.DataFrame:\n",
    "        self.class_to_id = {}\n",
    "        data = []\n",
    "        for path, _, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                if file.endswith(('.jpg', '.png', '.jpeg', '.bmp', '.gif')):\n",
    "                    class_name = os.path.basename(path)\n",
    "                    if class_name not in self.class_to_id:\n",
    "                        self.class_to_id[class_name] = len(self.class_to_id)\n",
    "                    data.append((f'{path}/{file}', self.class_to_id[class_name]))\n",
    "\n",
    "        return pd.DataFrame(data, columns=['imagepath', 'label'])\\\n",
    "    \n",
    "    def tf_load_image(self, image_path: str, crop_size: tuple[int, int] = (256, 256)):\n",
    "        image = tf.io.read_file(image_path)\n",
    "        image = tf.io.decode_image(image, expand_animations=False, channels=3)\n",
    "        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "        image = tf.image.resize(image, size=[*crop_size])\n",
    "        return self.augment(image) if self.augmentation else image\n",
    "    \n",
    "    def augment(self, image):\n",
    "        if tf.random.uniform((), maxval=1) > 0.5:\n",
    "            return tf.image.random_flip_left_right(image)\n",
    "        return tf.image.random_flip_up_down(image)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n // self.batch_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        items_from_dataset = self.dataframe[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "    \n",
    "        return tf.stack(list(map(self.tf_load_image, items_from_dataset.imagepath))), tf.stack(items_from_dataset.label)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        print(\"Hooray, IT'S ALIVE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3 = DatasetFromDirectory(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs3 = tf.keras.layers.Input((256, 256, 3), name='input')\n",
    "x3 = tf.keras.layers.Conv2D(32, 3, padding='same')(inputs3)\n",
    "x3 = tf.keras.layers.PReLU()(x3)\n",
    "x3 = tf.keras.layers.Flatten()(x3)\n",
    "x3 = tf.keras.layers.Dense(64)(x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = tf.keras.Model(inputs=inputs3, outputs=x3, name='ciaonima-1')\n",
    "model3.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics='accuracy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2808/2808 [==============================] - 487s 172ms/step - loss: nan - accuracy: 0.0019\n",
      "Hooray, IT'S ALIVE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2425b8ad0c0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(dataset3, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetFromCSV(tf.keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 path: str = './',\n",
    "                 batch_size: int = 32,\n",
    "                 image_size: tuple[int, int] = (256, 256),\n",
    "                 shuffle: bool = True,\n",
    "                 split: str = 'train',\n",
    "                 augmentation: bool = True\n",
    "                 ):\n",
    "        self.directory = path\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.shuffle = shuffle\n",
    "        self.split = split\n",
    "        self.augmentation = augmentation\n",
    "\n",
    "        self.dataframe = self.pd_from_csv(self.directory)\n",
    "        \n",
    "        if self.shuffle:\n",
    "            self.dataframe = self.dataframe.sample(frac=1)\n",
    "\n",
    "        if self.split:\n",
    "            self.dataframe = self.dataframe[self.dataframe['data set'] == self.split]\n",
    "\n",
    "        self.n = len(self.dataframe)\n",
    "\n",
    "    def pd_from_csv(self, path):\n",
    "        data = pd.read_csv(path)\n",
    "        data.filepaths = [os.path.join(os.path.dirname(path), file).replace('\\\\', '/') for file in data.filepaths.tolist()]\n",
    "        classes = pd.unique(data.labels)\n",
    "        class_to_id = {name: i for i, name in enumerate(classes)}\n",
    "        data.insert(3, 'labels_id', [class_to_id[name] for name in data.labels], True)\n",
    "        return data\n",
    "    \n",
    "    def tf_load_image(self, image_path: str, crop_size: tuple[int, int] = (256, 256)):\n",
    "        image = tf.io.read_file(image_path)\n",
    "        image = tf.io.decode_image(image, expand_animations=False, channels=3)\n",
    "        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "        image = tf.image.resize(image, size=[*crop_size])\n",
    "        return self.augment(image) if self.augmentation else image\n",
    "    \n",
    "    def augment(self, image):\n",
    "        if tf.random.uniform((), maxval=1) > 0.5:\n",
    "            return tf.image.random_flip_left_right(image)\n",
    "        return tf.image.random_flip_up_down(image)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n // self.batch_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        items_from_dataset = self.dataframe[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "    \n",
    "        return tf.stack(list(map(self.tf_load_image, items_from_dataset.filepaths))), tf.stack(items_from_dataset.labels_id)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        print(\"Hooray, IT'S ALIVE!?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset4 = DatasetFromCSV('./archive/new_birds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs4 = tf.keras.layers.Input((256, 256, 3), name='input')\n",
    "x4 = tf.keras.layers.Conv2D(32, 3, padding='same')(inputs4)\n",
    "x4 = tf.keras.layers.PReLU()(x4)\n",
    "x4 = tf.keras.layers.Flatten()(x4)\n",
    "x4 = tf.keras.layers.Dense(64)(x4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = tf.keras.Model(inputs=inputs4, outputs=x4, name='ciaonima-1')\n",
    "model4.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics='accuracy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2644/2644 [==============================] - 439s 166ms/step - loss: nan - accuracy: 0.0019\n",
      "Hooray, IT'S ALIVE!?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17c7bd4e650>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.fit(dataset4, epochs=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
